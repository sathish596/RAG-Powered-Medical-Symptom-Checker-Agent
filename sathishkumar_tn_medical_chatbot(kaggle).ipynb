{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3892155,
          "sourceType": "datasetVersion",
          "datasetId": 2308952
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "id": "DTfH0tyAQ-8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "SjM8rSwyQ-8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developing a Medical Chatbot Using RAG and LLMs\n",
        "\n",
        "## Name: SATHISH KUMAR TURUKACHENAHALLI NARAYANAREDDY\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "VZKx91xU3Ywz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "dyyca9d-Q-8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "9Vqpu_mOQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **❗️Disclaimer❗️**\n",
        "The chatbot developed in this project is **not a substitute for professional medical diagnosis**. Its responses are generated based solely on the dataset it was trained on, which is limited in scope and not clinically comprehensive. Please do not rely on its outputs for making medical decisions.\n",
        "\n",
        "Always consult a licensed healthcare provider for any health concerns.\n",
        "\n",
        "If you are in an emergency situation, please seek immediate medical attention. You can find a list of emergency contact numbers worldwide [in this link.](https://www.dt.com/ca/wp-content/uploads/2017/03/Global-_911_Emergency-Contacts.pdf)"
      ],
      "metadata": {
        "id": "uU1GgRQbQ-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "40ot6I86Q-8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Introduction\n",
        "\n",
        "**Motivation:** When people feel unwell, they often search online to understand their symptoms but the information they find can be confusing. Visiting a doctor isn’t always immediately possible, especially in remote areas. A medical chatbot can help bridge this gap by giving quick, easy-to-understand information about possible health conditions. It can guide users to make better decisions about whether to seek medical help, all from the comfort of their home.\n",
        "\n",
        "**What are LLMs?**\n",
        "A Large Language Model (LLM) is an advanced AI model that can understand and generate human-like text. It is trained on a large amount of text data and can answer questions, write content, summarize information and hold conversations.\n",
        "\n",
        "**Why LLMs?**\n",
        "LLMs can understand and generate natural human language, making them ideal for building chatbots. They are capable of handling complex queries, providing detailed responses and adapting to different ways people describe their symptoms."
      ],
      "metadata": {
        "id": "_vRsdJKJQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Outline\n",
        "\n",
        "1. Data Preprocessing\n",
        "2. Implementing a simple **rule-based** chatbot\n",
        "3. Developing a chatbot using **text embeddings** and **RAG**\n",
        "4. **Fine-tuning** LLMs for our specific data and use-case"
      ],
      "metadata": {
        "id": "pTeVP-hWQ-8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "nUcYTQnZQ-8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "7h2FXEcgQ-8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import all the necessary libraries which will be used for data preprocessing as well as moving forward in building the chatbot.\n",
        "\n",
        "The **warnings** library is used to supress unimportant warnings while we run the cells"
      ],
      "metadata": {
        "id": "RRDbRc69Q-8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pupy6zmRQ-8Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Reading the data\n",
        "\n",
        "We will import a dataset from Kaggle. This step does not require us to download the dataset and we can directly access it using the /kaggle/input path. For this we will first have to add the dataset to our Kaggle notebook.  \n",
        "\n",
        "**To add the data follow the steps below**:\n",
        "1. Click *Input* on the right menu bar\n",
        "2. Select *+ Add Input*\n",
        "3. Enter this URL in the search bar: https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data\n",
        "4. Click on the *+* icon to add the dataset to the notebook\n",
        "\n",
        "You can explore the data we are using for this project [here](https://www.kaggle.com/datasets/karthikudyawar/disease-symptom-prediction/data)\n",
        "\n",
        "In this project, we will only use **dataset.csv** which contains the disease and its corresponding symptoms list"
      ],
      "metadata": {
        "id": "S4iyPYVJQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df = pd.read_csv(\"/kaggle/input/disease-symptom-prediction/dataset.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "16yf0f9rQ-8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how the dataset is structured using the pandas `head` function"
      ],
      "metadata": {
        "id": "Zg6sLyfmQ-8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "iMiBd98PawCJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the symptoms have underscores which need to be removed\n",
        "\n",
        "#### Step 2: Remove Underscores from the Symptoms Text\n",
        "\n",
        "Steps to remove the underscores:\n",
        "1. Find all columns in dis_symp_df where the column names start with \"Symptom\"\n",
        "2. After finding those columns, replace the _ with a blank space"
      ],
      "metadata": {
        "id": "_J7Hcn5FkhPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "symptom_cols = [col for col in dis_symp_df.columns if col.startswith(\"Symptom\")]\n",
        "dis_symp_df[symptom_cols] = dis_symp_df[symptom_cols].replace(\"_\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "7mLnNQGkcX6P",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify if the underscores have been removed by printing the first few entries of the dataframe"
      ],
      "metadata": {
        "id": "EsRk8LsOQ-8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "VQEE0EAPkX-a",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transforming the Structure of the Dataset\n",
        "\n",
        "We need to transform this dataset to match input-output pairs that are suitable for training LLMs\n",
        "\n",
        "Two Methods:\n",
        "\n",
        "\n",
        "1.   Pivot Longer: will result in a bigger dataset and more suited for simple classification tasks\n",
        "2.   Comma-separated symptoms in one column: suitable for sentence-level input from users\n",
        "\n",
        "Therefore, we will proceed with combining the symptoms into one column and separating them with commas\n",
        "\n"
      ],
      "metadata": {
        "id": "734MMipLcMHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to create a comma-separated list of symptoms:\n",
        "\n",
        "\n",
        "1. Combine the symptom values row-wise  \n",
        "For each row, go through the values in the symptom_cols:\n",
        "* Skip any missing values\n",
        "* Join the non-missing symptom strings with commas\n",
        "* Store this in a new column called **Symptoms**\n",
        "\n",
        "2. Remove all the original symptoms columns"
      ],
      "metadata": {
        "id": "OKpEsnqDQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "dis_symp_df[\"Symptoms\"] = dis_symp_df[symptom_cols].apply(lambda row: ', '.join([s for s in row if isinstance(s, str)]), axis=1)\n",
        "dis_symp_df.drop(symptom_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "52j6BfrDlMFA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the updated dataframe again using the `head` function"
      ],
      "metadata": {
        "id": "Jk-QM5BLQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "dis_symp_df.head()"
      ],
      "metadata": {
        "id": "s9NwCLB9mq20",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Check for Duplicate Lists\n",
        "\n",
        "Obtain a summary of the dataset using `info` function"
      ],
      "metadata": {
        "id": "5dstfR8wQ-8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "dis_symp_df.info()"
      ],
      "metadata": {
        "id": "H7AeYZIgnmnv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "dis_symp_df = dis_symp_df.drop_duplicates(subset = ['Symptoms'])"
      ],
      "metadata": {
        "id": "2S3JYj1gnoHX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the summary of the dataset again to see if there were any duplicates. Report back on your conclusion."
      ],
      "metadata": {
        "id": "kFegXNooQ-8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "dis_symp_df.info()"
      ],
      "metadata": {
        "id": "HlQ7KxTgo8I6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The dataset now contains 2 columns with the symptom columns consist of common disease's symtomps seperated by comma without any duplicates***"
      ],
      "metadata": {
        "id": "qOo-3JZxpPGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "NUNwR6pZQ-8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 2: Rule-Based Chatbot (Cosine Similarity)"
      ],
      "metadata": {
        "id": "SeBqC4pl4Ma1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now be implementing one of the most basic versions of a chatbot: **a rule-based chatbot using cosine similarity**.\n",
        "\n",
        "**Cosine similarity** is a metric used to measure how similar two vectors are, regardless of their magnitude.\n",
        "\n",
        "A **rule-based chatbot** using cosine similarity identifies the most appropriate response by comparing the user’s input with a set of predefined statements and selecting the one with the highest semantic similarity based on cosine similarity of their vector embeddings."
      ],
      "metadata": {
        "id": "zKtXgjG7Q-8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Import required packages from `sklearn`\n",
        "\n",
        "`TfidfVectorizer`: Read up more on Term Frequency-Inverse Document Frequency (TF-IDF) [here](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/). This is used to convert text into numerical vectors based on how important each word is.\n",
        "\n",
        "`cosine_similarity`: Package used to measure how similar the user's symptom input is to each disease's symptom list."
      ],
      "metadata": {
        "id": "B5LUmH6zQ-8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "GHwuLmIK4P7x",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Group All Symptom Entries For Each Disease into a Single String"
      ],
      "metadata": {
        "id": "ytLWF2-AQ-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(s for s in x if isinstance(s, str))).reset_index()"
      ],
      "metadata": {
        "id": "KqjPU44A4VI2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Vectorize only the **Symptoms** column from dis_symp_df using `TfidfVectorizer` and `fit_transform()`"
      ],
      "metadata": {
        "id": "YXkxDB64Q-8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(dis_symp_df[\"Symptoms\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "x0Or1EPCQ-8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "wZlHWTKQQ-8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    # Welcome message for the user\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Continue asking the user for input until they enter 'exit' or 'quit'\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # Converts the user's input into a TF-IDF vector using the previously trained vectorizer\n",
        "        user_vec = vectorizer.transform([user_input])\n",
        "\n",
        "        # Compares the user's vector with all disease-symptom vectors in the tfidf_matrix using cosine similarity\n",
        "        # flatten() is used to convert the 2D result into a 1D array of vectors\n",
        "        cosine_sim = cosine_similarity(user_vec, tfidf_matrix).flatten()\n",
        "\n",
        "\n",
        "        # Sorts the similarity scores in descending order and retrieves the top 3 indices\n",
        "        top_indices = cosine_sim.argsort()[::-1][:3]\n",
        "\n",
        "        # Creates a list of (disease name, similarity score) tuples and only includes matches where the score is >0.5\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            if cosine_sim[i] > 0.2:\n",
        "                disease = dis_symp_df.iloc[i][\"Disease\"]\n",
        "                score = cosine_sim[i]\n",
        "                results.append(disease)\n",
        "\n",
        "        if not results:\n",
        "            print(\"ChatBot: I could not find a good match for your symptoms. Try rephrasing or listing more symptoms.\\n\")\n",
        "            continue\n",
        "\n",
        "        # If there are results, print the top-matching diseases with their similarity scores\n",
        "        print(\"ChatBot: Based on your symptoms, here are possible conditions:\")\n",
        "        for i, (disease) in enumerate(results, 1):\n",
        "            print(f\"   {i}. {disease}\")\n",
        "\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "MfXnIvff4jt6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "tziZARey7iW4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of Rule-Based Technique\n",
        "\n",
        "\n",
        "*   Does not generalize well to unseen data since there is no training involved\n",
        "*   Not scalable\n",
        "\n"
      ],
      "metadata": {
        "id": "uco9GYd-AIiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "qqj2hXb-Q-8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 3: Embeddings + RAG"
      ],
      "metadata": {
        "id": "uwgAP0vpAUoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings** are numerical representations of text that capture its meaning and semantic similarity in a vector space."
      ],
      "metadata": {
        "id": "QVASENgRQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over the Rule-Based Method\n",
        "* Flexible and Scalable: Unlike TF-IDF which depends on exact word matches, embedding-based retrieval finds relevant records based on context and similarity in meaning\n",
        "* More Robust: Since embeddings capture the semantic meaning behind words and generalize over language structure, minor spelling errors or synonymns do not affect performance, unlike TF-IDF which is sensitive to exact tokens\n",
        "* Context-Aware Responses: RAG combines retrieval with an LLM allowing it to generate human-like responses instead of returning pre-written text\n",
        "* Easier to Update Knowledge: New information can be added to the embedding database without retraining the LLM"
      ],
      "metadata": {
        "id": "VonKsE9WQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the project, we will be implementing a RAG-based chatbot using SentenceTransformer to create embeddings and the Llama-2 LLM to generate responses"
      ],
      "metadata": {
        "id": "KoC4MkBQQ-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** After running some of these cells, you may get warnings in a red box. Warnings are messages that alert us about possible issues in the code that aren't severe enough to stop execution. This is totally normal and you can still proceed with implementing the chatbot!"
      ],
      "metadata": {
        "id": "py1EuqT7Q-8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install the auto-gptq and optimum libraries\n",
        "\n",
        "**auto-gptq:** used for loading and running quantized versions of large language models efficiently (more on this in Milestone 4)  \n",
        "**optimum:** a library by Hugging Face that helps optimize model inference and training, particularly with quantized models"
      ],
      "metadata": {
        "id": "Lb8IasSXQ-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq optimum"
      ],
      "metadata": {
        "id": "E1SWpwPmTH1v",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**torch:** imports PyTorch, a popular deep learning framework used for model loading, tensor computations and training/inference  \n",
        "**transformers:** Hugging Face's library for working with pretrained models  \n",
        "**AutoTokenizer:** automatically loads the appropriate tokenizer for a given model  \n",
        "**AutoModelForCausalLM:** loads a causal language model (used for text generation)  \n",
        "**sentence_transformers:** a library for generating embeddings (vector representations) of sentences  \n",
        "**SentenceTransformer:** Loads a model to convert text into embeddings  \n",
        "**util:** Provides utility functions like `semantic_search()` for comparing embeddings."
      ],
      "metadata": {
        "id": "bEOd5Iz0Q-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "phb243S_AatK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Prepare the dataset for embeddings\n",
        "\n",
        "1. Group rows by disease, join all symptoms into one sentence and convert grouped result back into a dataframe using `reset_index()`\n",
        "2. Create a **Text** column with the combined disease and its respective symptom list\n",
        "3. Convert the **Text** column into a list called **corpus** which will be used for generating embeddings. The corpus is our knowledge base."
      ],
      "metadata": {
        "id": "s1_reunRQ-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "dis_symp_df = dis_symp_df.groupby(\"Disease\")[\"Symptoms\"].apply(lambda x: \", \".join(x)).reset_index()\n",
        "dis_symp_df[\"Text\"] = dis_symp_df.apply(lambda row: f\"Disease: {row['Disease']}. Symptoms: {row['Symptoms']}\", axis=1)\n",
        "corpus = dis_symp_df[\"Text\"].to_list()"
      ],
      "metadata": {
        "id": "jDOl20HOK1el",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Transform the corpus into vector embeddings\n",
        "\n",
        "1. Load the pre-trained embeddings model `all-MiniLM-L6-v2` from SentenceTransformer\n",
        "2. Convert each text entry in the corpus into its numerical representation using `encode` and set `convert_to_tensor` to **True** to ensure that the output is in the PyTorch tensor format"
      ],
      "metadata": {
        "id": "_UXcnOJ6Q-8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "corpus_embeddings = embed_model.encode(corpus, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "EUiEGwdzLYGt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Load the LLM and its tokenizer\n",
        "\n",
        "1. Specify the pre-trained model. Here, we will be using a quantized (compressed) version of Llama-2-7B-Chat model fine-tuned and optimized by the TheBloke using GPTQ. You can read up more about it in this [link](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ).\n",
        "2. Load the corresponding tokenizer using `AutoTokenizer`\n",
        "3. Load the Llama-2 model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "ZDh5wao4Q-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Llama-2?**  \n",
        "LLaMA-2 is a family of open-source LLMs developed by Meta designed for natural language understanding and generation tasks"
      ],
      "metadata": {
        "id": "IYFbPuOgQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\", # automatically distribute the model across the GPU\n",
        "    torch_dtype=torch.float16, # uses half-precision to save memory and improve speed\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "OkE9uVS7LjsF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Generate a response from the Llama-2 model\n",
        "\n",
        "1. Tokenize the prompt\n",
        "2. Generate the output using sampling paramaters such as `max_new_tokens`, `do_sample`, `temperature` and `top_p`\n",
        "3. Decode the response into a readable string using `decode`\n",
        "4. Remove the original prompt text and return only the generated response"
      ],
      "metadata": {
        "id": "F77YjmJwQ-8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Parameters:**  \n",
        "`max_new_tokens`: controls the length of the generated response  \n",
        "`do_sample`: enables sampling, picks the next token randomly based on the predicted probability distribution  \n",
        "`temperature`: a lower value gives a more factual response while a higher value could lead to potential hallucination  \n",
        "`top_p`: a higher value ensures the model avoids rare and low probability words"
      ],
      "metadata": {
        "id": "XZMc28QAQ-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "def generate_llama2_response(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3nGdPLuIQ-8s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Generate a response based on our dataset using RAG\n",
        "\n",
        "1. Convert the user query into embeddings using the same SentenceTransformer object (`embed_model`). This allows us to compare the input semantically with the knowledge base\n",
        "2. Perform semantic search using util's `semantic_search` function to find the top_k most similar records from **corpus_embeddings**\n",
        "3. Retrieve the actual text from the original corpus by matching the index\n",
        "4. Create an effective and descriptive prompt for the LLM\n",
        "5. Finally, pass the prompt to the Llama-2 function we defined above"
      ],
      "metadata": {
        "id": "sC1PvVCKQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "def rag_response(user_input):\n",
        "    query_embedding = embed_model.encode(user_input, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)[0]\n",
        "    retrieved_contexts = [corpus[hit[\"corpus_id\"]] for hit in hits]\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a medical assistant. Based on the medical records below, \"\n",
        "        f\"suggest top 2 possible diseases the user might have. Be concise and give the response in points.\\n\\n\"\n",
        "        \"Make sure to also include a disclaimer at the bottom telling users that this is not a medical diagnosis and they should always consult a doctor.\"\n",
        "        \"Medical Records:\\n\" + \"\\n\".join(retrieved_contexts) +\n",
        "        f\"\\n\\nUser Symptoms: {user_input}\\n\\nYour Response:\"\n",
        "    )\n",
        "\n",
        "    return generate_llama2_response(prompt)"
      ],
      "metadata": {
        "id": "HMZKanYnNbG9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Create a chatbot interface for the user"
      ],
      "metadata": {
        "id": "WpWKIE4GQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # call the rag_response function to obtain the Llama-2 generated output\n",
        "        response = rag_response(user_input)\n",
        "\n",
        "        print(f\"ChatBot: {response}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "id": "QK0oEb7KOEUJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The following cell may take some time to run because of embeddings generation and semantic search"
      ],
      "metadata": {
        "id": "cKvWJ7MaQ-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot interface\n",
        "chatbot()"
      ],
      "metadata": {
        "id": "vnx0IBpwSnOC",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Your Understanding!\n",
        "\n",
        "Time to try fine-tuning an LLM by yourself!  \n",
        "Let's use the BioMistral model once again since it is well-suited for medical applications.   \n",
        "We already have our formatted dataframe, so we will start off by loading the model."
      ],
      "metadata": {
        "id": "DnbNiIw3Q-8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step A: Load `BioMistral/BioMistral-7B` and its tokenizer\n",
        "\n",
        "Refer to Step 5 if you get stuck!"
      ],
      "metadata": {
        "id": "mQBOxW3VQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "model_id_new = \"BioMistral/BioMistral-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id_new, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id_new,\n",
        "    device_map=\"auto\", # automatically distribute the model across the GPU\n",
        "    torch_dtype=torch.float16, # uses half-precision to save memory and improve speed\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "liGsg-NNQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step B: Generate a response from the Mistral model\n",
        "\n",
        "Refer to Step 6 if you get stuck!"
      ],
      "metadata": {
        "id": "NnVV3HC5Q-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "def generate_mistral_response(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()"
      ],
      "metadata": {
        "trusted": true,
        "id": "L6R1w_YyQ-8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step C: Generate a response based on our dataset using RAG\n",
        "\n"
      ],
      "metadata": {
        "id": "Cxmoe9dAQ-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "def rag_response(user_input):\n",
        "    query_embedding = embed_model.encode(user_input, convert_to_tensor=True)\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=2)[0]\n",
        "    retrieved_contexts = [corpus[hit[\"corpus_id\"]] for hit in hits]\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a medical assistant. Based on the medical records below, \"\n",
        "        f\"suggest top 2 possible diseases the user might have. Be concise and give the response in points.\\n\\n\"\n",
        "        \"Make sure to also include a disclaimer at the bottom telling users that this is not a medical diagnosis and they should always consult a doctor.\"\n",
        "        \"Medical Records:\\n\" + \"\\n\".join(retrieved_contexts) +\n",
        "        f\"\\n\\nUser Symptoms: {user_input}\\n\\nYour Response:\"\n",
        "    )\n",
        "\n",
        "    return generate_mistral_response(prompt)"
      ],
      "metadata": {
        "trusted": true,
        "id": "d6zQ-uwuQ-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step D: Create a chatbot interface for the user\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RBfJaeDVQ-8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms ('fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\n Note: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        # call the rag_response function to obtain the Llama-2 generated output\n",
        "        response = rag_response(user_input)\n",
        "\n",
        "        print(f\"ChatBot: {response}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "kkTY8Pm4Q-8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disadvantages of RAG + Embeddings\n",
        "* Embedding generation, semantic search and LLM inference are resource-intensive and require longer compute times\n",
        "* Requires GPU for efficieny"
      ],
      "metadata": {
        "id": "WYDXUjK-Q-8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "ORM-pRzuQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Note on PEFT and Quantization in Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "sDB_zETbQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameter-Efficient Fine-Tuning (PEFT)**  \n",
        "PEFT techniques allow you to fine-tune LLMs by updating only a small subset of parameters rather than the entire model. This makes training more efficient and reduces hardware requirements, ideal when working with limited resources.  \n",
        "\n",
        "**Quantization**  \n",
        "Quantization means converting model weights from a high-memory format (like 32-bit floats) to a lower one (like 8-bit integers). This helps reduce memory usage and allows large models to run on devices with less RAM and smaller GPUs. It also makes inference faster. For example, models can be run on phones or laptops instead of needing expensive servers.  \n",
        "\n",
        "**LoRA (Low-Rank Adaptation)**  \n",
        "LoRA is a technique used during fine-tuning that avoids updating all of the model's weights. Instead, it learns small changes to the model and stores them separately. These changes are computed using two smaller matrices, which means fewer parameters need to be updated. This makes training much faster and lighter.  \n",
        "\n",
        "**QLoRA**  \n",
        "QLoRA combines quantization and LoRA. It compresses model weights to 4-bit precision and then fine-tunes the model using LoRA. This lets you fine-tune large models using much less memory without sacrificing too much performance"
      ],
      "metadata": {
        "id": "9Hard2RiQ-8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next milestone, we will look into implementing QLoRA to fine-tune Llama-2 effectively to meet the GPU constraints"
      ],
      "metadata": {
        "id": "afM6jDR9Q-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "JjwZBqyRQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 4: Fine-Tuning LLMs"
      ],
      "metadata": {
        "id": "VSQJKAVdUeVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does fine-tuning LLMs mean?**\n",
        "* Fine-tuning means adapting a pre-trained LLM to perform better on a specific task by continuing its training on a domain-specific dataset\n",
        "* The LLM learns patterns in the dataset and adjusts its internal weights slightly to adapt to that domain to get more relevant responses\n",
        "* For example, in our case, a base model like Llama-2 may just know general health facts but after fine-tuning it on our disease-symptom dataset, it will give more accurate answers\n",
        "\n",
        "In this section of the project, we will be fine-tuning LLMs for our medical chatbot"
      ],
      "metadata": {
        "id": "PAdekVRIQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages over Embeddings + RAG Method\n",
        "* Better Domain Alignment: Fine-tuning tailors the model to specifc domain knowledge improving accuracy\n",
        "* Faster Inference: Without a retrieval step, fine-tuned models can respond faster"
      ],
      "metadata": {
        "id": "VDzmg5OcQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Install required libraries\n",
        "\n",
        "**peft:** enables parameter-efficient training for large models  \n",
        "**datasets:** used to convert a pandas dataframe into a format that is compatible with Hugging Face's Trainer  \n",
        "**accelerate:** simeplifies mixed-precision training  \n",
        "**bitsandbytes:** enables quantization to reduce memory usage when training large models"
      ],
      "metadata": {
        "id": "764L_7oVQ-8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Before running the cell below, please restart the session. You can do this by clicking the 3 dots on the upper right-hand corner and selecting *Restart & Clear Cell Outputs*. An error message might appear as you run the cell below, but you can carry on with the project without worrying about it!"
      ],
      "metadata": {
        "id": "-Pk7N6Y5Q-8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell only after restarting the session\n",
        "!pip install -q peft datasets accelerate bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "id": "7vQkyzdlQ-8x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Note:** Since the session has restarted, the dataset is no longer available. Please return to Milestone 1, run all the cells in that section to reload the data, and then come back to Milestone 4 once you are done!"
      ],
      "metadata": {
        "id": "KjHxNjTTQ-8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Import necessary packages\n",
        "\n",
        "**TrainingArguments:** specifies training parameters for the LLM  \n",
        "**Trainer:** training loop abstraction to simplify model training  \n",
        "**BitsAndBytesConfig:** used for quantized training  \n",
        "**LoraConfig:** defines the configuration for LoRA fine-tuning  \n",
        "**get_peft_model:** wraps a base model with PEFT (LoRA) layers  \n",
        "**prepare_model_for_kbit_training:** prepares a model for 4-bit or 8-bit training  \n",
        "**PeftModel:** to load a LoRA-trained model for inferencing"
      ],
      "metadata": {
        "id": "202X7bz7Q-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft import PeftModel\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "_1hkQAsuQ-8y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Transform the dataset into required format for fine-tuning Llama-2\n",
        "\n",
        "Llama models generally require a specific format as the input which is the `[INST] ... [/INST]` format.  \n",
        "For example, we need to transform our dataset to look like this:\n",
        "`<s>[INST] abdominal pain, fever [\\INST] Appendicitis`  \n",
        "\n",
        "1. Define a function that formats the input passed into the format we discussed above\n",
        "2. Apply the `format_prompt` function to each row of the dataframe and create a new column called **text** that stores the formatted prompt for each row\n",
        "3. Convert the dataframe into a Hugging Face `Dataset` object"
      ],
      "metadata": {
        "id": "ai_tO-AIQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "def format_prompt(row):\n",
        "    return f\"<s>[INST] {row['Symptoms']} [/INST] {row['Disease']}\"\n",
        "\n",
        "dis_symp_df[\"text\"] = dis_symp_df.apply(format_prompt, axis=1)\n",
        "\n",
        "formatted_df = Dataset.from_pandas(dis_symp_df)"
      ],
      "metadata": {
        "id": "gyCd86rNUh8y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out how the first entry of `formatted_df` looks"
      ],
      "metadata": {
        "id": "2igRNRbVQ-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "formatted_df['Symptoms'][0]"
      ],
      "metadata": {
        "id": "Lss2W1h2dmud",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Load the Llama-2 chat model using QLoRA\n",
        "\n",
        "1. Specify the Hugging Face model we want to load. Here we will be using `NousResearch/Llama-2-7b-chat-hf` which is a 7B parameter version of Llama-2\n",
        "2. Set up the 4-bit quantization for QLoRA using `BitsAndBytesConfig` and define the parameter values\n",
        "3. Load the quantized model using `AutoModelCausalLM`"
      ],
      "metadata": {
        "id": "sSx0EnwwQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},  # explicitly use GPU 0 (GPU T4 x2)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "U_sXc_hdQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Set up LoRA for a 4-bit quantized Llama-2 model\n",
        "\n",
        "1. Prepare the 4-bit quantized model for training using `prepare_model_for_kbit_training`\n",
        "2. Create the configuration for LoRA and define the parameter values\n",
        "3. Wrap the model with LoRA using the defined configuration. This resuts in only a small set of trainable weights which reduces compute and memory needs"
      ],
      "metadata": {
        "id": "HuYtRf1zQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # rank of the LoRA update matrices\n",
        "    lora_alpha=16, # scaling factor for the LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust based on model architecture, here we apply LoRA only to the query and value projection layers of attention\n",
        "    lora_dropout=0.1, # dropout applied to LoRA layers during training to avoid overfitting\n",
        "    bias=\"none\", # do not train the bias parameters\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUf-azeyQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Tokenize Dataset\n",
        "\n",
        "1. Load the corresponding tokenizer for our model\n",
        "2. Set the `pad_token` to be the same as the `eos_token` since models like Llama do not have separate padding token defined by default"
      ],
      "metadata": {
        "id": "AcekV-2SQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ww85KMVdQ-8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define function that takes one row and processes it for training\n",
        "4. Using `tokenizer` convert the input text into token IDs\n",
        "5. Set labels to be a copy of input_ids. In causal language modeling, the model is trained to predict the next token so the input and out are the same"
      ],
      "metadata": {
        "id": "WNcE7qLlQ-8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "trusted": true,
        "id": "jwt1-Y_lQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Apply the `tokenize_function` to each row of the `formatted_df` and remove columns **text**, **Disease** and **Symptoms** to keep only the tokenized inputs"
      ],
      "metadata": {
        "id": "-Q8d04FCQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "tokenized_datasets = formatted_df.map(tokenize_function, remove_columns=[\"text\", \"Disease\", \"Symptoms\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ClkRdHSCQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Define training parameters for fine-tuning the Llama-2 model"
      ],
      "metadata": {
        "id": "NaUB6C6dQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    run_name=\"llama2-finetune\",\n",
        "    report_to=\"none\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=1,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    max_steps=-1,\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    save_steps=0\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "nxcXrvOKQ-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Initialize `Trainer`\n",
        "\n",
        "Define the following parameters:  \n",
        "**model:** the LoRA-wrapped Llama-2 model we are fine-tuning  \n",
        "**args:** the training arguments we defined above  \n",
        "**train_dataset:** the tokenized dataset that contains the formatted and encoded input-output pairs  \n",
        "**tokenizer:** the tokenizer used to process inputs and decode outputs to ensure consistency between training and generation"
      ],
      "metadata": {
        "id": "0BKAyL0LQ-80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YBLRr4V0Q-80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 9: Train your LLM\n",
        "\n",
        "Finally, after the preprocessing and parameter definition, we can train our LLM!"
      ],
      "metadata": {
        "id": "LIVCm3w4Q-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qR89mmpsQ-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 10: Model Inferencing\n",
        "\n",
        "Now that we have our fine-tuned LLM, we will use it to predict possible diseases for different user inputs.\n",
        "\n",
        "1. Save the fine-tuned Llama-2 model and tokenizer to a specific directory in the Kaggle environment"
      ],
      "metadata": {
        "id": "9ADxb4OXQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/kaggle/working/llama2-med-chatbot\"\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {output_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "WOx1oDLZQ-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the base model, calling the original Llama-2 model `NousResearch/Llama-2-7b-chat-hf`\n",
        "3. Load the tokenizer from `output_dir`, set `pad_token` to `eos_token` and set `padding_side` to right which is standard for causal language models"
      ],
      "metadata": {
        "id": "Lr7JZa0gQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "trusted": true,
        "id": "mH96P718Q-81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load the base model with quantization using the bitsandbytes configuration define above"
      ],
      "metadata": {
        "id": "b3_LiqwbQ-81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "8ZboWzGJQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Attach the LoRA fine-tuned weights from `output_dir` and merge them with the base model using `PeftModel`\n",
        "6. Set the model to evaluation using the `eval` function to put the model in inference mode"
      ],
      "metadata": {
        "id": "8P0sL-1TQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the blanks\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8XEwOHEZQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create the chatbot interface function for the user"
      ],
      "metadata": {
        "id": "MMakv1nVQ-82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"ChatBot: I can help suggest possible diseases based on your symptoms.\")\n",
        "    print(\"Type your symptoms (e.g., 'fever, cough, sore throat'), or type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"ChatBot: Goodbye!\\nNote: This is not a medical diagnosis. Always consult a licensed physician.\")\n",
        "            break\n",
        "\n",
        "        instruction = \"List the top 2 possible diseases for these symptoms:\"\n",
        "        # formatting the prompt using the required Llama-2 structure\n",
        "        prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{instruction}\n",
        "<</SYS>>\n",
        "\n",
        "Symptoms: {user_input} [/INST]\"\"\"\n",
        "\n",
        "        # converts prompt into token IDs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # to generate response from the model with key parameters\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=300,\n",
        "                do_sample=False,\n",
        "                temperature=0.2,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # decode the output tokens into readable text\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # extracts only the relevant part after [/INST] which contains the response\n",
        "        if \"[/INST]\" in full_response:\n",
        "            answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "        else:\n",
        "            answer = full_response.strip()\n",
        "\n",
        "        print(f\"ChatBot: {answer}\\n\")\n",
        "        print(\"Note: This is not a medical diagnosis. Always consult a licensed physician.\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "36AKil0eQ-82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Jn19r_mnQ-83"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
